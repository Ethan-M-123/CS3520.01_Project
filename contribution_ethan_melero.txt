Team Contributions File: The Lazy Chefs

Nick Contributions:
    Worked on merging the model and knowledge base together, Nickolas was the one who wrote the majority of the code, some with me and Brandon's assistance. However he is an incredibly hardworking team mate and we would not have been able to do this project without his dedication and effort. I cannot stress enough how vital Nick was to the project, he is the one who set up the image classification and basic object detection versions whole cloth, and with those we were able to take those and evolve them into what our final product is today. 

Brandon Contributions:
    Filled the prolog knowledge base of recipes for the app to reference, searched for all the urls for each of the recipes in the knowledge base, as well as wrote python scripts to help with the modelâ€™s dataset (out of the 961 images his scripts were able to grab me 733 which gave me big head start on model training). I also praise Brandon for this tenacity to get as much time as possible on this project, without his pestering this would've been due the day we presented and none of this would've been possible to complete on time. 

Ethan Melero Contributions:
    Within this project I trained the object detection model and manually labeled all of the 961 training images for the 50 different ingredients for the model. The model was worked through a total of 6 iterations, the first 2 mainly being tests for learning how CustomVision worked and setting the grounds for how many images it needed to work decently as by a default it required at least 15 images per tag, and the latter 4 being the actual working models we would use for the project, each iteration increasing the recall and precision as this was always the main problem id be faced with when working on the model. The minimum of 15 images per tag proved insufficient as some ingredients overlapped in how they looked or would be overshadowed by other ingredients that they looked similar to. (such as it mistaking eggs for marshmallows or vice-versa) so the only work around for this was simply adding more images to the dataset for it to train off of, which was beneficial to the model anyways as this would improve its precision and recall. Though other than model working I also assisted Nick in the actual programming of the project, such as helping with bug fixing. And I also didn't do all the image searching myself as Brandon made some python scripts to scrub google images for my dataset (though a large majority of these images proved to be not incredibly useful, as not every image is nice for the model to train off of), I still however did lots of personal image scrubbing to grab quality training images that wouldn't just further confuse the model.

(Sorry for the big block of text, just didn't know how else to summarize my contributions as model training isn't very quantifiable in terms of github commits so felt the need to explain why mine are so low and it was mainly because of the fact that github actually has a file upload size limit of 25Mb of which the models would typically be 40Mb so uploading them to github wouldn't work and as such I would upload them onto a google drive folder that I shared with the group; here it is if you need it: https://drive.google.com/drive/folders/1H2xzoCkuXVKxJv8ShLin50WmWyVILehV?usp=sharing )

